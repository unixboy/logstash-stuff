input {
  lumberjack {
    port => 1471
    ssl_certificate => "/etc/pki/tls/certs/logstash-forwarder.crt"
    ssl_key => "/etc/pki/tls/private/logstash-forwarder.key"
  }
}

filter {
  if [type] == "apache_access" {
    grok {
      match => { "message" => "%{IPORHOST:loadbalancer} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)( %{USER:auth})? (?:%{IPORHOST:clientip}|-) %{QS:referrer} %{QS:agent}"}
    }
    date {
      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    geoip {
      source => "clientip"
      target => "geoip"
      database => "/etc/logstash/GeoLiteCity.dat"
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
      replace => [ "message", "%{response} %{verb} %{request}"]
    }
  }
}
filter {
  if [type] == "backstage_stash" {
    json {
      source => "message"
    }
    mutate {
      rename => { "level" => "loglevel"
                  "logger_name" => "module"
      }
      remove_field => [ "HOSTNAME", "level_value", "thread_name" ] 
    }
  }
}
filter {
  if [type] == "tomcat_access" {
    grok {
      match => { "message" => "%{IPORHOST:loadbalancer} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)( %{IPORHOST:clientip})?" }
    }
    date {
      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    mutate {
      replace => [ "message", "%{response} %{verb} %{request}"]
    }
  }
}
filter {
  if [type] == "openam" {
     grok {
       match => { "message" => "\"%{TIMESTAMP_ISO8601:timestamp}\"%{SPACE}(?:%{QS:data}|%{NOTSPACE:data})%{SPACE}(?:%{QS:contextid}|%{NOTSPACE:contextid})%{SPACE}(?:%{QS:loginid}|%{NOTSPACE:loginid})%{SPACE}(?:%{IPORHOST:clientip}|\"Not Available\")%{SPACE}%{WORD:loglevel}%{SPACE}(?:%{QS:domain}|%{NOTSPACE:domain})%{SPACE}%{NOTSPACE:messageid}%{SPACE}(?:%{QS:loggedby}|%{NOTSPACE:loggedby})%{SPACE}(?:%{QS:nameid}|%{NOTSPACE:nameid})%{SPACE}%{NOTSPACE:modulename}%{SPACE}(?:%{IPORHOST:hostname})%{GREEDYDATA}" }
    }
    mutate {
      replace => [ "message", "%{data}"]
    }
    date {
      match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss" ]
    }
  }
}
filter {
  if [type] == "elasticsearch" {
    multiline {
      pattern => "^%{TIMESTAMP_ISO8601} "
      negate => true
      what => previous
    }
    grok {  # parses the common bits
      match => [ "message", "\[%{TIMESTAMP_ISO8601:timestamp}\]\[%{DATA:severity}%{SPACE}\]\[%{DATA:source}%{SPACE}\]%{SPACE}\[%{DATA:node}\]%{SPACE}(?<message>(.|\r|\n)*)" ]
      overwrite => [ "message" ]
    }
    date { # use timestamp from the log
      "match" => [ "timestamp", "YYYY-MM-dd HH:mm:ss,SSS" ]
      target => "@timestamp"
    }
  }
}
filter {
  if [type] == "couchdb" {
    grok {
      match => { "message" => "\[%{GREEDYDATA:timestamp}\] \[%{GREEDYDATA:loglevel}\] \[%{GREEDYDATA:pid}\] %{IP:clientip} (?:%{USER:ident}|-) (?:%{USER:auth}|-) %{WORD:verb} %{NOTSPACE:request} %{NUMBER:response}" }
    }
    date {
      match => [ "timestamp" , "EEE, dd MMM yyyy HH:mm:ss z" ]
    }
    mutate {
      replace => [ "message", "%{response} %{verb} %{request}"]
    }
  }
}
filter {
  if [type] == "amagent" {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => { "message" => "%{AGENT_TIME:timestamp}%{SPACE}%{WORD:loglevel}%{SPACE}%{NUMBER:pid}\:%{NOTSPACE:thread}%{SPACE}%{DATA:module}\:%{SPACE}%{GREEDYDATA:data}"}
    }
    date {
      match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss.SSS" ]
    }
    mutate {
      replace => [ "message", "%{data}"]
    }
  }
}
filter {
  if [type] == "haproxy" {
    grok {
      patterns_dir => "/etc/logstash/patterns"
      match => { "message" => "%{HAPROXY_TIME:timestamp} %{WORD:nodename} haproxy\[%{NUMBER:pid}\]\: %{GREEDYDATA:data}"}
    }
    date {
      match => [ "timestamp" , "MMM dd HH:mm:ss" ]
    }
    mutate {
      replace => [ "message", "%{data}"]
    }
  }
}
filter {
  ruby {
    code => "
      event['timefield'] = event['@timestamp'].to_i
      event['micros'] = event.sprintf('%{+SSS}')
      event['padoffset'] = event['offset'].to_s.rjust(12,'0')
    "
  }
  mutate {
    add_field => [ "orderfield", "%{timefield}%{micros}%{padoffset}"  ]
  }
}
output {
  elasticsearch {
    host  => "localhost"
    port => 9200
    protocol => "http"
    cluster => "backstage-monitor"
    index => "logstash-%{+YYYY.MM.dd}"
  }
}

